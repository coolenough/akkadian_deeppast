{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d61d945",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-09T21:11:42.373173Z",
     "iopub.status.busy": "2026-01-09T21:11:42.372867Z",
     "iopub.status.idle": "2026-01-09T21:12:18.378868Z",
     "shell.execute_reply": "2026-01-09T21:12:18.377921Z"
    },
    "papermill": {
     "duration": 36.013962,
     "end_time": "2026-01-09T21:12:18.382692",
     "exception": false,
     "start_time": "2026-01-09T21:11:42.368730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-09 21:12:00.094797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767993120.333202      23 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767993120.404856      23 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767993120.973427      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767993120.973460      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767993120.973463      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767993120.973465      23 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Count: 2\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Clear memory first\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Hardware check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8050f359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:12:18.389308Z",
     "iopub.status.busy": "2026-01-09T21:12:18.388347Z",
     "iopub.status.idle": "2026-01-09T21:12:18.397742Z",
     "shell.execute_reply": "2026-01-09T21:12:18.397073Z"
    },
    "papermill": {
     "duration": 0.013912,
     "end_time": "2026-01-09T21:12:18.399181",
     "exception": false,
     "start_time": "2026-01-09T21:12:18.385269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_FILE = \"/kaggle/input/deep-past-initiative-machine-translation/train.csv\"\n",
    "\n",
    "MODEL_PATH = \"/kaggle/working/flant5_small_final_optimized\"\n",
    "\n",
    "\n",
    "MAX_LENGTH = 384       \n",
    "BATCH_SIZE = 10         \n",
    "GRAD_ACCUMULATION = 8  \n",
    "LEARNING_RATE = 1e-3   \n",
    "NUM_EPOCHS = 3\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5ebbac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:12:18.405132Z",
     "iopub.status.busy": "2026-01-09T21:12:18.404464Z",
     "iopub.status.idle": "2026-01-09T21:12:18.411892Z",
     "shell.execute_reply": "2026-01-09T21:12:18.411347Z"
    },
    "papermill": {
     "duration": 0.01178,
     "end_time": "2026-01-09T21:12:18.413201",
     "exception": false,
     "start_time": "2026-01-09T21:12:18.401421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    \n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    \n",
    "    text = text.replace(\"…\", \"...\")\n",
    "    text = re.sub(r'\\.\\s*\\.\\s*\\.', '...', text) \n",
    "    \n",
    "    \n",
    "    text = text.replace('“', '\"').replace('”', '\"')\n",
    "    \n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def is_valid_translation(text):\n",
    "    \n",
    "    if not text or len(text) < 2:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    clean_check = re.sub(r'[^\\w\\s]', '', text)\n",
    "    if len(clean_check) == 0:\n",
    "        return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def remove_noisy(df):\n",
    "    oare_ids = []\n",
    "    with open('/kaggle/input/outliers/oare_ids_outilers_iqr.txt','r') as f:\n",
    "        oare_ids = [line.strip() for line in f.readlines()]\n",
    "    mask = ~df['oare_id'].isin(oare_ids)\n",
    "    return mask\n",
    "    \n",
    "def preprocess_dataframe(df):\n",
    "    print(f\"Original Row Count: {len(df)}\")\n",
    "    \n",
    "    \n",
    "    df['transliteration'] = df['transliteration'].apply(clean_text)\n",
    "    df['translation'] = df['translation'].apply(clean_text)\n",
    "    df_clean = df[remove_noisy(df)]\n",
    "    \n",
    "    valid_mask = df['translation'].apply(is_valid_translation)\n",
    "    df_cleaned = df[valid_mask].copy()\n",
    "    \n",
    "    print(f\"Cleaned Row Count: {len(df_clean)}\")\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows (empty/broken translations)\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74f47e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:12:18.418791Z",
     "iopub.status.busy": "2026-01-09T21:12:18.418297Z",
     "iopub.status.idle": "2026-01-09T21:12:18.431486Z",
     "shell.execute_reply": "2026-01-09T21:12:18.430343Z"
    },
    "papermill": {
     "duration": 0.017226,
     "end_time": "2026-01-09T21:12:18.432647",
     "exception": true,
     "start_time": "2026-01-09T21:12:18.415421",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/flant5_small_final_optimized'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/flant5_small_final_optimized'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/2011861888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    906\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/flant5_small_final_optimized'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Prefix\n",
    "    inputs = [\"translate Akkadian to English: \" + str(ex) for ex in examples[\"transliteration\"]]\n",
    "    targets = [str(ex) for ex in examples[\"translation\"]]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True,\n",
    "        padding=False # Dynamic padding is faster\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, \n",
    "        max_length=MAX_LENGTH, \n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622f56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:18:59.272717Z",
     "iopub.status.busy": "2026-01-09T19:18:59.272050Z",
     "iopub.status.idle": "2026-01-09T19:18:59.469934Z",
     "shell.execute_reply": "2026-01-09T19:18:59.469165Z",
     "shell.execute_reply.started": "2026-01-09T19:18:59.272677Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    local_files_only=True,\n",
    "    use_safetensors=True \n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883c15d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T19:19:13.205437Z",
     "iopub.status.busy": "2026-01-09T19:19:13.205151Z",
     "iopub.status.idle": "2026-01-09T19:19:13.239723Z",
     "shell.execute_reply": "2026-01-09T19:19:13.239165Z",
     "shell.execute_reply.started": "2026-01-09T19:19:13.205411Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"byt5_small_akkadian_optimized\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    \n",
    "    # MEMORY SAVING SETTINGS\n",
    "    per_device_train_batch_size=BATCH_SIZE,        # 2\n",
    "    per_device_eval_batch_size=BATCH_SIZE,         # 2\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION, # 8\n",
    "    gradient_checkpointing=True,                   # <--- THE MAGIC FIX (Saves 50% VRAM)\n",
    "    optim=\"adafactor\",                             # <--- Uses less memory than AdamW\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,                  \n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f881452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T20:22:46.099814Z",
     "iopub.status.busy": "2026-01-09T20:22:46.099123Z",
     "iopub.status.idle": "2026-01-09T21:00:04.874275Z",
     "shell.execute_reply": "2026-01-09T21:00:04.873376Z",
     "shell.execute_reply.started": "2026-01-09T20:22:46.099786Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "\n",
    "df = preprocess_dataframe(df)\n",
    "history = []\n",
    "kfold = KFold(n_splits = 5,shuffle = True,)\n",
    "for train_indices,val_indices in tqdm(kfold.split(df)):\n",
    "    train_df = df.iloc[train_indices,:]\n",
    "    val_df = df.iloc[val_indices,:]\n",
    "\n",
    "\n",
    "    raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df)\n",
    "    })\n",
    "    \n",
    "    print(\"Sample Transliteration:\", train_df.iloc[0]['transliteration'],'\\n')\n",
    "    print(\"Sample Translation:\", train_df.iloc[0]['translation'])\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    "    )\n",
    "    print(\"Tokenization Complete.\")\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting Memory-Optimized Training...\")\n",
    "    trainer.train()\n",
    "    # SAVE\n",
    "    final_path = \"./flant5_small_final_optimized\"\n",
    "    trainer.save_model(final_path)\n",
    "    tokenizer.save_pretrained(final_path)\n",
    "    print(f\"Saved model to {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0801d5e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-01-09T16:09:37.639350Z",
     "iopub.status.idle": "2026-01-09T16:09:37.639582Z",
     "shell.execute_reply": "2026-01-09T16:09:37.639484Z",
     "shell.execute_reply.started": "2026-01-09T16:09:37.639470Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d35fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:00:16.326892Z",
     "iopub.status.busy": "2026-01-09T21:00:16.326227Z",
     "iopub.status.idle": "2026-01-09T21:00:16.787589Z",
     "shell.execute_reply": "2026-01-09T21:00:16.786991Z",
     "shell.execute_reply.started": "2026-01-09T21:00:16.326863Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_path = \"./flanT5_small_CV_optimized\"\n",
    "trainer.save_model(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "print(f\"Saved model to {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2641e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:02:52.925290Z",
     "iopub.status.busy": "2026-01-09T21:02:52.924558Z",
     "iopub.status.idle": "2026-01-09T21:02:53.166023Z",
     "shell.execute_reply": "2026-01-09T21:02:53.165427Z",
     "shell.execute_reply.started": "2026-01-09T21:02:52.925261Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TEST_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\n",
    "MODEL_PATH = \"/kaggle/working/flant5_small_final_optimized\" \n",
    "\n",
    "BATCH_SIZE = 16   # Safe inference batch size\n",
    "MAX_LENGTH = 384  # Must match training length\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- CLEANING FUNCTION (Must match training!) ---\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    text = text.replace(\"…\", \"...\")\n",
    "    text = re.sub(r'\\.\\s*\\.\\s*\\.', '...', text)\n",
    "    text = text.replace('“', '\"').replace('”', '\"')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(f\"Loading model from {MODEL_PATH}...\")\n",
    "\n",
    "# Load Model Offline\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/flant5_small_final_optimized', local_files_only=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/flant5_small_final_optimized', local_files_only=True)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b438b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:03:02.585038Z",
     "iopub.status.busy": "2026-01-09T21:03:02.584438Z",
     "iopub.status.idle": "2026-01-09T21:03:08.811349Z",
     "shell.execute_reply": "2026-01-09T21:03:08.810463Z",
     "shell.execute_reply.started": "2026-01-09T21:03:02.585010Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = [clean_text(t) for t in df['transliteration']]\n",
    "        # Add prefix\n",
    "        self.texts = [\"translate Akkadian to English: \" + t for t in self.texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ids = df['id'].tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=MAX_LENGTH, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"id\": self.ids[idx]\n",
    "        }\n",
    "\n",
    "# Load Test Data\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "test_dataset = InferenceDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "all_predictions = []\n",
    "all_ids = []\n",
    "\n",
    "print(f\"Starting Inference on {len(test_df)} rows...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        # Beam Search for best translation quality\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_LENGTH,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        all_predictions.extend([d.strip() for d in decoded])\n",
    "        all_ids.extend(batch[\"id\"].tolist())\n",
    "\n",
    "print(\"Inference Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992da7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-09T21:03:08.812899Z",
     "iopub.status.busy": "2026-01-09T21:03:08.812669Z",
     "iopub.status.idle": "2026-01-09T21:03:08.828379Z",
     "shell.execute_reply": "2026-01-09T21:03:08.827702Z",
     "shell.execute_reply.started": "2026-01-09T21:03:08.812864Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": all_ids,\n",
    "    \"translation\": all_predictions\n",
    "})\n",
    "\n",
    "# Final Sanity Check: Fill empty predictions if any exist\n",
    "submission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(x) > 0 else \"...\")\n",
    "\n",
    "# Save\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv saved.\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d4a8fb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15061024,
     "sourceId": 121150,
     "sourceType": "competition"
    },
    {
     "datasetId": 9219135,
     "sourceId": 14433440,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9227929,
     "sourceId": 14446529,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 575,
     "modelInstanceId": 3046,
     "sourceId": 4250,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 575,
     "modelInstanceId": 3045,
     "sourceId": 4251,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42.258411,
   "end_time": "2026-01-09T21:12:21.857721",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-09T21:11:39.599310",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
