{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"sourceType":"competition"},{"sourceId":14433440,"sourceType":"datasetVersion","datasetId":9219135},{"sourceId":14446529,"sourceType":"datasetVersion","datasetId":9227929},{"sourceId":14458096,"sourceType":"datasetVersion","datasetId":9234740},{"sourceId":4250,"sourceType":"modelInstanceVersion","modelInstanceId":3046,"modelId":575},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045,"modelId":575},{"sourceId":692580,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":525132,"modelId":539167},{"sourceId":714652,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":543118,"modelId":556282}],"dockerImageVersionId":31236,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport unicodedata\nimport pandas as pd\nimport numpy as np\nimport torch\nimport gc\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    DataCollatorForSeq2Seq, \n    Seq2SeqTrainingArguments, \n    Seq2SeqTrainer\n)\nfrom datasets import Dataset, DatasetDict\n\n# Clear memory first\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Hardware check\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\nif device == \"cuda\":\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:18:42.777606Z","iopub.execute_input":"2026-01-18T11:18:42.777955Z","iopub.status.idle":"2026-01-18T11:19:14.560820Z","shell.execute_reply.started":"2026-01-18T11:18:42.777929Z","shell.execute_reply":"2026-01-18T11:19:14.560094Z"}},"outputs":[{"name":"stderr","text":"2026-01-18 11:18:56.998087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768735137.197757      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768735137.251906      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768735137.751146      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768735137.751183      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768735137.751187      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768735137.751189      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nGPU Count: 2\nGPU Name: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nTRAIN_FILE = \"/kaggle/input/deep-past-initiative-machine-translation/train.csv\"\n\nMODEL_PATH = '/kaggle/input/akkadian-byt5-translator/pytorch/default/1/akkadian_byt5_trained'\n\n\nMAX_LENGTH = 384       \nBATCH_SIZE = 20         \nGRAD_ACCUMULATION = 8  \nLEARNING_RATE = 0.25e-3   \nNUM_EPOCHS = 5\nSEED = 42\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:19:14.562633Z","iopub.execute_input":"2026-01-18T11:19:14.563144Z","iopub.status.idle":"2026-01-18T11:19:14.572771Z","shell.execute_reply.started":"2026-01-18T11:19:14.563116Z","shell.execute_reply":"2026-01-18T11:19:14.571994Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def clean_text(text):\n    if pd.isna(text):\n        return \"\"\n    \n    text = str(text)\n    \n    \n    text = unicodedata.normalize('NFC', text)\n    \n    \n    text = text.replace(\"…\", \"...\")\n    text = re.sub(r'\\.\\s*\\.\\s*\\.', '...', text) \n    \n    \n    text = text.replace('“', '\"').replace('”', '\"')\n    \n    \n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\ndef is_valid_translation(text):\n    \n    if not text or len(text) < 2:\n        return False\n    \n    \n    clean_check = re.sub(r'[^\\w\\s]', '', text)\n    if len(clean_check) == 0:\n        return False\n        \n    return True\n\ndef remove_noisy(df):\n    oare_ids = []\n    with open('/kaggle/input/outliers/oare_ids_outilers_iqr.txt','r') as f:\n        oare_ids = [line.strip() for line in f.readlines()]\n    mask = ~df['oare_id'].isin(oare_ids)\n    return mask\n    \ndef preprocess_dataframe(df):\n    print(f\"Original Row Count: {len(df)}\")\n    \n    \n    df['transliteration'] = df['transliteration'].apply(clean_text)\n    df['translation'] = df['translation'].apply(clean_text)\n    df_clean = df[remove_noisy(df)]\n    \n    valid_mask = df['translation'].apply(is_valid_translation)\n    df_cleaned = df_clean[valid_mask].copy()\n    \n    print(f\"Cleaned Row Count: {len(df_clean)}\")\n    print(f\"Dropped {len(df) - len(df_clean)} rows (empty/broken translations)\")\n    \n    return df_cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:19:14.592374Z","iopub.execute_input":"2026-01-18T11:19:14.592682Z","iopub.status.idle":"2026-01-18T11:19:14.605944Z","shell.execute_reply.started":"2026-01-18T11:19:14.592649Z","shell.execute_reply":"2026-01-18T11:19:14.605280Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n\n\ndef preprocess_function(examples):\n    # Prefix\n    inputs = [\"translate Akkadian to English: \" + str(ex) for ex in examples[\"transliteration\"]]\n    targets = [str(ex) for ex in examples[\"translation\"]]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        inputs, \n        max_length=MAX_LENGTH, \n        truncation=True,\n        padding=False # Dynamic padding is faster\n    )\n    \n    # Tokenize targets\n    labels = tokenizer(\n        text_target=targets, \n        max_length=MAX_LENGTH, \n        truncation=True,\n        padding=False\n    )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:19:14.606729Z","iopub.execute_input":"2026-01-18T11:19:14.607014Z","iopub.status.idle":"2026-01-18T11:19:14.643371Z","shell.execute_reply.started":"2026-01-18T11:19:14.606978Z","shell.execute_reply":"2026-01-18T11:19:14.642844Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(f\"Loading model from {MODEL_PATH}...\")\n\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL_PATH, \n    local_files_only=True,\n    use_safetensors=True \n)\nmodel = model.to(device)\n\n# Collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T11:19:14.645009Z","iopub.execute_input":"2026-01-18T11:19:14.645295Z","iopub.status.idle":"2026-01-18T11:19:42.596673Z","shell.execute_reply.started":"2026-01-18T11:19:14.645273Z","shell.execute_reply":"2026-01-18T11:19:42.595996Z"}},"outputs":[{"name":"stdout","text":"Loading model from /kaggle/input/akkadian-byt5-translator/pytorch/default/1/akkadian_byt5_trained...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"args = Seq2SeqTrainingArguments(\n    output_dir=\"flan-T5-small-optimised\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    \n    # MEMORY SAVING SETTINGS\n    per_device_train_batch_size=BATCH_SIZE,        # 2\n    per_device_eval_batch_size=BATCH_SIZE,         # 2\n    gradient_accumulation_steps=GRAD_ACCUMULATION, # 8\n    gradient_checkpointing=True,                   # <--- THE MAGIC FIX (Saves 50% VRAM)\n    optim=\"adafactor\",                             # <--- Uses less memory than AdamW\n    lr_scheduler_type='cosine',\n    weight_decay=0.02,\n    save_total_limit=1,\n    num_train_epochs=NUM_EPOCHS,\n    predict_with_generate=True,\n    metric_for_best_model=\"bleu\",\n    fp16=True,                  \n    report_to=\"none\",\n    load_best_model_at_end=True,\n    logging_steps=5,\n\n    generation_max_length=MAX_LENGTH,        \n    generation_num_beams=4,\n)\n\nimport numpy as np\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n\n    # 1. Decode predictions\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # 2. Decode labels (replace -100 padding with pad_token_id)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # 3. NLTK Formatting: Tokenize and wrap references in a list\n    # BLEU expects: \n    #   References: [[['ref1_word1', 'ref1_word2']], [['ref2_word1', ...]]]\n    #   Candidates: [['cand1_word1', 'cand1_word2'], ['cand2_word1', ...]]\n    formatted_preds = [pred.strip().split() for pred in decoded_preds]\n    formatted_labels = [[label.strip().split()] for label in decoded_labels]\n\n    # 4. Calculate BLEU with smoothing\n    # Smoothing is important early in training when 4-gram matches are rare\n    smoothie = SmoothingFunction().method1\n    score = corpus_bleu(formatted_labels, formatted_preds, smoothing_function=smoothie)\n\n    return {\"bleu\": score * 100} # Scale to 0-100 like standard benchmarks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:59:03.567776Z","iopub.execute_input":"2026-01-18T15:59:03.568106Z","iopub.status.idle":"2026-01-18T15:59:03.612860Z","shell.execute_reply.started":"2026-01-18T15:59:03.568077Z","shell.execute_reply":"2026-01-18T15:59:03.612284Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from sklearn.model_selection import KFold\ndf = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\n\n\ntrain_df,val_df = train_test_split(df,test_size = 0.3)\n\n\nraw_datasets = DatasetDict({\n\"train\": Dataset.from_pandas(train_df),\n\"validation\": Dataset.from_pandas(val_df)\n    \n})\n    \n    \nprint(\"Sample Transliteration:\", train_df.iloc[0]['transliteration'],'\\n')\nprint(\"Sample Translation:\", train_df.iloc[0]['translation'])\n\ntokenized_datasets = raw_datasets.map(\npreprocess_function, \nbatched=True,\nremove_columns=raw_datasets[\"train\"].column_names\n)\nprint(\"Tokenization Complete.\")\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics = compute_metrics\n    )\n\nprint(\"Starting Memory-Optimized Training...\")\ntrainer.train()\n    # SAVE\nfinal_path = \"./flant5_small_final_optimized\"\ntrainer.save_model(final_path)\ntokenizer.save_pretrained(final_path)\nprint(f\"Saved model to {final_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:59:12.721952Z","iopub.execute_input":"2026-01-18T15:59:12.722698Z"}},"outputs":[{"name":"stdout","text":"Sample Transliteration: šu-ma um-me-a-an ša-lim-a-šur a-na AN.NA ù x x x na ší TÚG ša en-na-nim x zu ú en-um-a-šur a-na en-na-nim i-tù-ru ma-lá AN.NA ù (TÚG)ṣú-ba-tí ša qá-tim i-na ba-áb DINGIR-lim i-za-ku-ú iš-tí um-mì-a-an ša-lim-a-šur en-na-nam DUMU am-ri-a en-um-a-šur ú-ba-áb IGI tù-ur-a-a IGI lá-qé-ep \n\nSample Translation: If Šalim-Ašsur's investors raise claim against Ennānum concerning tin ... textiles belonging to Ennānum concerning tin ... textiles belonging to Ennānum ... and Ennam-Aššur, then Ennam-Aššur will release Ennānum son of Amriya with Šalim-Aššur's investors for as much tin and ordinary textiles as they clear at the gate of the God. Witnessed by Turaya, by Lā-qēp.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1092 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb864506d45d414ead56fcb4df220daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/469 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d457fa31caf46999a73240549fc11ee"}},"metadata":{}},{"name":"stdout","text":"Tokenization Complete.\nStarting Memory-Optimized Training...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2796278204.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='17' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [17/20 1:18:51 < 15:46, 0.00 it/s, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.297040</td>\n      <td>37.510941</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.189200</td>\n      <td>0.290125</td>\n      <td>43.350021</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.170600</td>\n      <td>0.299471</td>\n      <td>43.867495</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='6' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6/12 03:29 < 04:11, 0.02 it/s]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"final_path = \"./flanT5_small_CV_optimized\"\ntrainer.save_model(final_path)\ntokenizer.save_pretrained(final_path)\nprint(f\"Saved model to {final_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2026-01-16T05:23:35.555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport unicodedata\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# --- CONFIGURATION ---\nTEST_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/test.csv\"\nMODEL_PATH = \"/kaggle/working/flant5_small_final_optimized\" \n\nBATCH_SIZE = 16   # Safe inference batch size\nMAX_LENGTH = 384  # Must match training length\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# --- CLEANING FUNCTION (Must match training!) ---\ndef clean_text(text):\n    if pd.isna(text): return \"\"\n    text = str(text)\n    text = unicodedata.normalize('NFC', text)\n    text = text.replace(\"…\", \"...\")\n    text = re.sub(r'\\.\\s*\\.\\s*\\.', '...', text)\n    text = text.replace('“', '\"').replace('”', '\"')\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\nprint(f\"Loading model from {MODEL_PATH}...\")\n\n# Load Model Offline\ntry:\n    tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/flant5_small_final_optimized', local_files_only=True)\n    model = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/flant5_small_final_optimized', local_files_only=True)\n    model = model.to(DEVICE)\n    model.eval()\n    print(\"✅ Model loaded successfully!\")\nexcept Exception as e:\n    print(f\"❌ Error loading model: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T05:50:51.251764Z","iopub.execute_input":"2026-01-16T05:50:51.252565Z","iopub.status.idle":"2026-01-16T05:50:51.531146Z","shell.execute_reply.started":"2026-01-16T05:50:51.252532Z","shell.execute_reply":"2026-01-16T05:50:51.530331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InferenceDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.texts = [clean_text(t) for t in df['transliteration']]\n        # Add prefix\n        self.texts = [\"translate Akkadian to English: \" + t for t in self.texts]\n        self.tokenizer = tokenizer\n        self.ids = df['id'].tolist()\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        inputs = self.tokenizer(\n            text, \n            padding=\"max_length\", \n            truncation=True, \n            max_length=MAX_LENGTH, \n            return_tensors=\"pt\"\n        )\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"id\": self.ids[idx]\n        }\n\n# Load Test Data\ntest_df = pd.read_csv(TEST_PATH)\ntest_dataset = InferenceDataset(test_df, tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nall_predictions = []\nall_ids = []\n\nprint(f\"Starting Inference on {len(test_df)} rows...\")\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        \n        # Beam Search for best translation quality\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_LENGTH,\n            num_beams=4,\n            early_stopping=True\n        )\n        \n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        all_predictions.extend([d.strip() for d in decoded])\n        all_ids.extend(batch[\"id\"].tolist())\n\nprint(\"Inference Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T05:50:54.884082Z","iopub.execute_input":"2026-01-16T05:50:54.884428Z","iopub.status.idle":"2026-01-16T05:51:00.909695Z","shell.execute_reply.started":"2026-01-16T05:50:54.884401Z","shell.execute_reply":"2026-01-16T05:51:00.908919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\nsubmission = pd.DataFrame({\n    \"id\": all_ids,\n    \"translation\": all_predictions\n})\n\n# Final Sanity Check: Fill empty predictions if any exist\nsubmission[\"translation\"] = submission[\"translation\"].apply(lambda x: x if len(x) > 0 else \"...\")\n\n# Save\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv saved.\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T05:51:07.860257Z","iopub.execute_input":"2026-01-16T05:51:07.860609Z","iopub.status.idle":"2026-01-16T05:51:07.876368Z","shell.execute_reply.started":"2026-01-16T05:51:07.860583Z","shell.execute_reply":"2026-01-16T05:51:07.875665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}